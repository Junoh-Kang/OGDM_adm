# project:
log_dir: './logs'
project: "test"
exp: "00_tdep_disc"
log: []

# data:
data_dir: "./data/celeba_64"
image_size: 64

# model:
num_channels: 128
num_res_blocks: 3
num_heads: 1
num_heads_upsample: -1
num_head_channels: -1
attention_resolutions: "16,8"
channel_mult: ""
dropout: 0.0
class_cond: False
use_scale_shift_norm: True
resblock_updown: False
use_fp16: False
use_new_attention_order: False

# discriminator:
use_discriminator: True
t_dim: 1

# diffusion:
learn_sigma: False
diffusion_steps: 1000
noise_schedule: "linear"
timestep_respacing: ""
use_kl: False
predict_xstart: False
rescale_timesteps: False
rescale_learned_sigmas: False
schedule_sampler: "uniform"
# "uniform" / "disc_aware_uniform:loss_target=0.7,ema_rate=0.9" / "disc_aware_priority:loss_target=0.7,ema_rate=0.9" 

# training:
# resume
use_checkpoint: False
resume_checkpoint: ""
# batch
batch_size: 2
microbatch: -1  # -1 disables microbatches
# about losses
lr_model: 1e-4
lr_disc: 1e-4
lossD_type: "logistic"
lossG_weight: 0.0
grad_weight: 1.0
# saving
ema_rate: '0.9999'
log_interval: 10
save_interval: 10000
sample_num: 8
sample_type: "ddim200"
# etc
fp16_scale_growth: 0.001
weight_decay: 0.0
lr_anneal_steps: 0 